<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Isabella Alpert & Robert Nochez</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-top-dog-enjoyers-1/hw3/">https://cal-cs184-student.github.io/hw-webpages-top-dog-enjoyers-1/hw3/</a><br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-woopers-woops-1">https://github.com/cal-cs184-student/sp25-hw3-woopers-woops-1</a>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		
		<p>
			Throughout this homework, we learned how to implement the core processes of a physically-based renderer using a pathtracing 
			algorithm. We first accomplished this through ray generation and scene intersection, where we generated rays via sampling 
			and implemented certain intersections with different geometries--in this case, spheres and triangles. Next, we implemented 
			Bounding Volume Hierarchy or BVH. We did this through implementing a BVH construction algorithm. This allows us to keep 
			track of all of the primitives in a scene, resulting in increased traversal efficiency. In Part 3, we implemented direct 
			illumination and in Part 4, we implemented global illumination. These two tasks taught us how to light objects that have a 
			diffuse BSDF. We also implemented heuristics like Russian Roulette and compared uniform hemisphere sampling and importance 
			lighting sampling to see how different methodologies result in different lighting effects. Finally, in Part 5, we implemented 
			adaptive sampling, a heuristic that uses a fixed, high number of samples per pixel. As it concentrates the samples in the 
			more difficult parts of the image, allowing us to decrease noise without uniformly sampling the scene. By completing this 
			homework, we learned different tactics and ways to render images while also noticing the drawbacks of each tactic in terms of 
			performance, noise, and quality.
		</p>

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		
		<p>
			Ray generation works by taking normalized coordinates in an image space with normalized coordinates turning them into 
			a sensor in the camera space. This works by taking pixel coordinates, transforming them using <code>hFov</code> and 
			<code>vFov</code> as points of reference, storing the transformed coordinates into a vector, and then normalizing it.
			Next, we generate the ray in the camera space by multipling <code>c2w</code> and the previously mentioned vector and 
			normalizing the result. Finally, we return the ray, creating it using the camera's position, our new vector, and 
			<code>min_t</code> and <code>max_t</code>. Through ray generation, we are able to check for primitive intersection.
			Primitive intersection works by generating rays and testing them against geometry--in this case, triangles and 
			spheres--for intersections. The specifics of triangle intersection can be seen below. Sphere intersection works 
			similarly to triangle intersection, with the primary difference being that we solve for the quadratic formula, set 
			it equal to <code>t</code>, and test if it falls between <code>min_t</code> and <code>max_t</code>. For spheres, we
			must also define <code>t1</code> and <code>t2</code> due to there being multiple intersections. For both intersections,
			we must update the intersection information stored in the intersection parameter accordingly.
		</p>

		<p>
			The triangle intersection algorithm that we implemented was the MÃ¶ller Trumbore Algorithm from Lectures 9/10. The 
			algorithm works by first calculating the edges of the triangles using the three points given in 
			<code>mesh->positions</code> through <code>p2 - p1</code> and <code>p3 - p1</code>. Next, we calculate the 
			vector from the triange's vertex to the ray's origin using <code>r.o - p1</code> and store it in <code>s</code>. We 
			then take the cross product of the ray's direction and the triangle's second edge, storing the result in 
			<code>s1</code>, and the cross product of <code>s</code> and the triangle's first edge, storing the result in 
			<code>s2</code>. Afterwards, we create a <code>Vector3D</code> where the first entry is the dot product of 
			<code>s2</code> and <code>edge2</code>, the second entry is the dot product of <code>s1</code> and <code>s</code>,
			and the third entry is the dot product of <code>s2</code> and the ray's direction. We find the normalization term to
			be the one over the cross product of <code>s1</code> and <code>edge1</code> and multiply this by the Vector3D. This 
			results in a Vector3D that stores t, the scalar distance between the ray's origin and the intersection point, and 
			two barycentric coordinates. We can use these results to determine if there is an intersection by checking if 
			<code>t &gt; r.min_t || t &lt; r.max_t || b1 &lt; 0 || b2 &lt; 0 || b1 + b2 &gt; 1</code> are all false. Ultimately,
			we know that they do not intersect if the barycentric coordinates are less than 0, meaning they are outside of the 
			triangle, and if t is between <code>min_t</code> and <code>max_t</code>.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task1 - cbcoil.dae.png" width="300px"/>
					<figcaption>Rendering CBcoil.dae</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task1 - CBspheres_lambertian.dae.png" width="300px"/>
				  <figcaption>Rendering CBspheres_lambertian.dae</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task1 - cbgems.dae.png" width="300px"/>
					<figcaption>Rendering CBgems.dae</figcaption>
				  </td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>

		<p>
			Our BVH Construction algorithm begins with us looping through the given vector of primitives. Similar to the 
			starter code, we build a BVH aggregate with a single leaf node, the root, that encloses all the primitives and 
			count the number of primitives. Next we initialize a new BVHNode pointer named <code>node</code>, passing in the 
			root bounding box we just created. Afterwards, we check if the number of primitives is less than or equal to the 
			max leaf size. If true, then we know that we have a leaf node. We set <code>node->start = start</code> and 
			<code>node->end = end</code>, updating the pointers of our BVHNode accordingly. Finally, we return <code>node</code>.
			When this line of code is true, it also serves as the base case for our recursive call. If it is false, we need to 
			perform the recursive algorithm to construct BVH. The first step of this process is dividing the primitives into a 
			left and right collection. We do this by performing a series of conditionals using <code>bbox.extent.x</code>,
			<code>bbox.extent.y</code>, and <code>bbox.extent.z</code>. We compare them amongst each other, searching for the 
			best axis to split upon and store the axis that is greater than the other two in <code>double bestAxis</code>. Next, 
			we use <code>std::sort</code>, an in-line function that sorts the elements in the range [first, last) in 
			non-descending order. For this function, we pass in a comparison function object, <code>bestAxis</code>, and two 
			primitive pointers named a and b. Inside of the function, we check if the centroid of the bounding box along the 
			bestAxis for a is less than the centroid of the bounding box along the bestAxis for b. Through this, we are able to
			sort all of the primitives inside of the bounding box, helping us understand where to split for the left and right 
			collection. Using this information, we create a <code>std::vector&lt;Primitive *&gt;::iterator</code> named midpoint
			that finds the average of start plus the number of primitives to find the midpoint of our bounding box. Finally, we
			set the left and right pointers of <code>node</code> equal to the recursive call of <code>construct_bvh</code>. 
			For the left node, we pass in start, midpoint, and max_leaf_size. For the right node, we pass in midpoint, end, and
			max_leaf_size. Finally, we return node. For this implementation, we chose to use the midpoint of the bounding box 
			as our heuristic it attempts to create a balanced tree by splitting the primitives at the midpoint of their 
			bounding box along the longest axis, which we've entitled as bestAxis. By creating a balanced tree, we reduce 
			tree depth, allowing for a faster treversal.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task2 - lucy.png" width="400px"/>
					<figcaption>Rendering CBlucy.dae</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2 - max.png" width="400px"/>
				  <figcaption>Rendering maxplanck.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			In the table below, we can see the differences in runtime when rendering CBlucy.dae and maxplanck.dae with and 
			without using BVH acceleration. For the CBlucy.dae file, we can see that that without BVH acceleration, the image 
			took 577.2773 seconds, nearly 10 minutes, to render, while it took only 0.0666 seconds with it. Likewise, for the 
			maxplanck.dae file, it took 162.5172 seconds to render without acceleration and 0.0812 seconds with it. From this,
			we can see that we can see that BVH acceleration dramatically reduces rendering time by not performining 
			unnecessary intersection tests.  Without BVH, the ray tracing algorithm checks every primitive in the scene, leading 
			to an exponential increase in rendering time as scene complexity grows. In contrast, BVH partitions the scene, 
			allowing rays to quickly discard large groups of primitives that do not intersect. This results in speedups of 
			several orders of magnitude.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task2 - unoptimized lucy.png" width="400px"/>
					<figcaption>Unoptimized Lucy Runtime</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2 - optimized lucy.png" width="400px"/>
				  <figcaption>Optimized Lucy Runtime</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
					<img src="task2 - unoptimized max.png" width="400px"/>
					<figcaption>Unoptimized Max Runtime</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2 - optimized max.png" width="400px"/>
				  <figcaption>Optimized Max Runtime</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<h2>Part 3: Direct Illumination</h2>

		<p>
			Our implementation of direct lighting with uniform hemisphere sampling first starts with the given starter code.
			This code creates the world-to-object and object-to-world matrices, creates vectors <code>hit_p</code> and 
			<code>w_out</code>, and determines the number of samples we want to take of our scene. Next, we implement our sampling 
			loop. The loop runs <code>num_samples</code> times. We first obtain <code>w_i</code>, a randomly sampled vector above
			the hit point. We then convert this vector to the world space by multipling it with the object-to-world matrix. Next,
			we create a ray whose origin is the <code>hit_p + (EPS_F * w_i_world</code> and direction <code>w_i_world</code>. We 
			set this as the origin to ensure is being cast at the hit point, but add the second part to avoid numerical 
			precision issues. We then create an Intersection object named <code>worldIntersection</code>. Afterwards, we check 
			if this instance's BVH intersects with the ray and store the result in <code>worldIntersection</code>. If there is 
			an intersection, we use a Monte Carlo estimator to estimate how much light arrived at the intersection point. We 
			calculate each part of the estimator and store it in a variable. From the equation in the spec, we can see that 
			\( f_r(\mathbf{p}, \omega_j \to \omega_r) \) is the passed in intersection's BDSF using <code>f()</code>. 
			\( L_i(\mathbf{p}, \omega_j) \) is the emission of <code>worldIntersection</code>'s BDSF. \( \cos\theta_j \) is the dot product 
			of <code>w_i</code> and the normalized vector <code>(0, 0, 1)</code>. Finally, \( p(\omega_j) \) is <code>2.0 * PI</code>. 
			We use all of these to compute the Monte Carlo Estimator and add the result to <code>L_out</code>. After we have 
			finshed sampling, we divide <code>L_out</code> by the number of samples and return it.
		</p>
			
		<p>
			Our implementation of direct lighting with importance sampling lights is very similar to our implementation of uniform
			hemisphere sampling. It starts with the same starter code that creates the world-to-object and object-to-world matrices 
			and the <code>hit_p</code> and <code>w_out</code> vectors. Next, we loop through all of the lights within the scene. We 
			check if a light is a point light source. If it is, we only sample from it once. If not, we sample  
			<code>ns_area_light</code> times. For each light within the scene, we sample it <code>num_samples</code> times. We 
			create <code>Vector3D w_i</code>, <code>double distToLight</code>, and <code>double pdf</code>, variables that will 
			all be defined when we pass them into <code>sample_L()</code>. <code>sample_L</code> works by sampling the light, outputting
			that light's radiance, and then defining the three previously mentioned variables. As <code>w_i</code> is now in the world 
			space, we multiply it by the world-to-object matrix. Next, we create a ray whose origin is <code>hit_p + (EPS_F * w_i</code>
			and direction <code>w_i</code>, following a similar process and reasoning to the ray created in uniform hemisphere 
			sampling. However, for this ray, we set <code>ray.max_t = distToLight - EPS_F</code> and <code>ray.min_t = EPS_F</code> 
			to ensure the hit point is in a shadow with respect to the current light source.  We then create an Intersection object
			named <code>worldIntersection</code>. Afterwards, we check if this instance's BVH does not intersect with the ray and store the 
			result in <code>worldIntersection</code> and check if <code>w_i_obj.z >= 0</code>, checking if the light is behind the 
			surface at the hit point. If true, we use a Monte Carlo estimator to estimate how much light has arrived at the intersection 
			point. rom the equation in the spec, we can see that \( f_r(\mathbf{p}, \omega_j \to \omega_r) \) is the passed in intersection's 
			BDSF using <code>f()</code>. \( \cos\theta_j \) is the dot product of <code>w_i_obj</code> and the normalized vector 
			<code>(0, 0, 1)</code>. Finally, we use <code>f_r * radiance * cos_theta_i / pdf</code> to compute the Monte Carlo estimator
			and add it to <code>L_out</code>. After we have fully sampled this light <code>num_samples</code> times, we divide 
			<code>L_out</code> by <code>num_samples</code>. Finally, after we have sampled every light in the scene, we return 
			<code>L_out</code>.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task3 - hemi low samp.png" width="400px"/>
					<figcaption>Uniform Hemisphere Sampling with Low Rate</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3 - hemi high samp.png" width="400px"/>
				  <figcaption>Uniform Hemisphere Sampling with High Rate</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
					<img src="task3 - importance low samp.png" width="400px"/>
					<figcaption>Uniform Importance Sampling with Low Rate</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3 - importance high samp.png" width="400px"/>
				  <figcaption>Uniform Importance Sampling with High Rate</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			In the table below, we can see the differences in noise levels in the soft shadows when rendering the dragon using
			different amounts of light rays and one sample. The image is always slightly pixelated, most likely due to the 
			fact that we are only sampling one pixel so it is never smoothed out through averaging in an anti-aliasing-like effect.
			Through this, we observe that noise levels in soft shadows decrease as the number of samples per light increases.
			Despite this, we can clearly see the noise levels decrease as we go from using 1 lighr ray, to 4, to 16, and finally to 64,
			creating a smoother image overall.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task3 - sample 1.png" width="400px"/>
					<figcaption>Dragon with 1 Light Ray and 1 Sample</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3 - sample 4.png" width="400px"/>
				  <figcaption>Dragon with 4 Light Rays and 1 Sample</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
					<img src="task3 - sample 16.png" width="400px"/>
					<figcaption>Dragon with 16 Light Rays and 1 Sample</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3 - sample 64.png" width="400px"/>
				  <figcaption>Dragon with 64 Light Rays and 1 Sample</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			The major differences between uniform hemisphere sampling and lighting sampling are the amount of noise and the 
			efficiency of the algorithms. To further elaborate, uniform hemisphere sampling distributes samples uniformly over 
			the hemisphere, which has noisier shadows and requires a large number of samples to converge. In contrast, lighting 
			sampling focuses on light sources, reducing variance by focusing samples where illumination is most relevant. This 
			results in scenes with smoother and lessy noisy shadows with fewer samples than uniform hemisphere sampling.
		</p>

		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		
		<p>
			Our implementation for this part starts with us defining several variables, including <code>Vector3D radiance(0, 0, 0)</code>
			for the accumulated radiance, <code>int num_samples = 0</code> for the actual samples taken, <code>double s1 = 0.0</code> for 
			the sum of the illuminance values, and <code>double s2 = 0.0</code> for the sum of squared illuminance. Next, we loop 
			number of samples allocated (<code>ns_aa</code>) times. In this loop, we generate a sample from gridSampler, normalize the 
			X and Y coordinates of the sample, and then generate a ray from the camera using the normalized coordinates. We set the depth 
			of this ray to <code>this->max_ray_depth</code>. Next, we need to compute the radiance for this sample. We do this using 
			<code>est_radiance_global_illumination</code>, passing in <code>sample_ray</code>. We add this sampled radiance to our total 
			radiance vector and add one to <code>num_samples</code>. Afterwards, we need to compute the illuminance and update our 
			running statistics. We do this by calling <code>illum()</code> on our sampled radiance, add this value to <code>s1</code>, 
			and add this value squared to <code>s2</code>. Now, we need to check the convergence of every <code>samplesPerBatch</code>
			samples. We do this by checking if <code>num_samples % samplesPerBatch == 0</code> and if we've sampled more than once. If 
			true, we take the mean, variance, and standard deviation of our samples. We then create a 95% confidence interval. If the 
			pixel has converged, we stop early. Finally, after we have finished sampling, we update the pixel in the sample buffer and 
			increase the sample count buffer. 
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task5 - bunny.png" width="400px"/>
					<figcaption>Bunny with s = 2048</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task5 - bunny rate.png" width="400px"/>
				  <figcaption>Sampling Rate of Bunny</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
					<img src="task5 - spheres.png" width="400px"/>
					<figcaption>Spheres with s = 2048</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task5 - spheres rate.png" width="400px"/>
				  <figcaption>Sampling Rate of Spheres</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		</div>
	</body>
</html>